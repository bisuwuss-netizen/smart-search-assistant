"""
RAG è¯„ä¼°æ¨¡å—

æä¾›å¯¹æ£€ç´¢å’Œç”Ÿæˆè´¨é‡çš„è¯„ä¼°æŒ‡æ ‡ï¼š
1. æ£€ç´¢è¯„ä¼°ï¼šPrecision, Recall, MRR, NDCG
2. ç”Ÿæˆè¯„ä¼°ï¼šFaithfulness, Answer Relevancy
3. ç«¯åˆ°ç«¯è¯„ä¼°ï¼šAnswer Correctness

ä½¿ç”¨æ–¹å¼ï¼š
    python -m src.evaluation.rag_evaluator

å‚è€ƒæ¡†æ¶ï¼šRAGAS (https://github.com/explodinggradients/ragas)
"""
import json
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from src.config import Config
from src.utils.llm_factory import LLMFactory


@dataclass
class EvaluationResult:
    """å•ä¸ªè¯„ä¼°ç»“æœ"""
    question: str
    expected_answer: Optional[str]
    generated_answer: str
    contexts: List[str]
    # è¯„ä¼°æŒ‡æ ‡
    faithfulness: float  # ç­”æ¡ˆæ˜¯å¦åŸºäºæ£€ç´¢å†…å®¹
    answer_relevancy: float  # ç­”æ¡ˆä¸é—®é¢˜çš„ç›¸å…³æ€§
    context_precision: float  # æ£€ç´¢ç²¾ç¡®åº¦
    context_recall: float  # æ£€ç´¢å¬å›ç‡ï¼ˆéœ€è¦ground truthï¼‰


@dataclass
class EvaluationReport:
    """è¯„ä¼°æŠ¥å‘Š"""
    total_samples: int
    avg_faithfulness: float
    avg_answer_relevancy: float
    avg_context_precision: float
    avg_context_recall: float
    results: List[EvaluationResult]


class RAGEvaluator:
    """
    RAG ç³»ç»Ÿè¯„ä¼°å™¨

    ä½¿ç”¨ LLM ä½œä¸ºè¯„åˆ¤è€…ï¼ˆLLM-as-a-Judgeï¼‰æ¥è¯„ä¼°ï¼š
    - Faithfulness: ç­”æ¡ˆæ˜¯å¦å¿ å®äºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
    - Answer Relevancy: ç­”æ¡ˆæ˜¯å¦å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
    - Context Precision: æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦ç²¾ç¡®ç›¸å…³
    - Context Recall: æ˜¯å¦æ£€ç´¢åˆ°äº†æ‰€æœ‰å¿…è¦çš„ä¿¡æ¯
    """

    def __init__(self):
        self.llm = LLMFactory.get_qwen_model()

    def evaluate_faithfulness(self, answer: str, contexts: List[str]) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆå¿ å®åº¦

        æ£€æŸ¥ç­”æ¡ˆä¸­çš„é™ˆè¿°æ˜¯å¦éƒ½èƒ½åœ¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ä¾æ®
        """
        if not contexts:
            return 0.0

        context_text = "\n\n".join(contexts[:5])  # é™åˆ¶é•¿åº¦

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªç­”æ¡ˆè´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆçš„"å¿ å®åº¦"ï¼ˆFaithfulnessï¼‰ã€‚

## æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
{context_text}

## ç”Ÿæˆçš„ç­”æ¡ˆ
{answer}

## è¯„ä¼°æ ‡å‡†
å¿ å®åº¦è¡¡é‡ç­”æ¡ˆä¸­çš„ä¿¡æ¯æ˜¯å¦éƒ½èƒ½åœ¨ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ä¾æ®ã€‚
- 1.0: ç­”æ¡ˆå®Œå…¨åŸºäºä¸Šä¸‹æ–‡ï¼Œæ²¡æœ‰ç¼–é€ ä¿¡æ¯
- 0.7-0.9: å¤§éƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡ï¼Œå°‘é‡åˆç†æ¨æ–­
- 0.4-0.6: éƒ¨åˆ†åŸºäºä¸Šä¸‹æ–‡ï¼Œéƒ¨åˆ†å¯èƒ½æ˜¯ç¼–é€ 
- 0.0-0.3: å¤§é‡ä¿¡æ¯æ— æ³•ä»ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°ä¾æ®

è¯·åªè¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•°å­—ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰ï¼š"""

        response = self.llm.invoke([HumanMessage(content=prompt)])
        try:
            score = float(response.content.strip())
            return max(0.0, min(1.0, score))
        except:
            return 0.5

    def evaluate_answer_relevancy(self, question: str, answer: str) -> float:
        """
        è¯„ä¼°ç­”æ¡ˆç›¸å…³æ€§

        æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦çœŸæ­£å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜
        """
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªç­”æ¡ˆè´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹ç­”æ¡ˆå¯¹é—®é¢˜çš„"ç›¸å…³æ€§"ï¼ˆAnswer Relevancyï¼‰ã€‚

## ç”¨æˆ·é—®é¢˜
{question}

## ç”Ÿæˆçš„ç­”æ¡ˆ
{answer}

## è¯„ä¼°æ ‡å‡†
ç›¸å…³æ€§è¡¡é‡ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚
- 1.0: å®Œç¾å›ç­”äº†é—®é¢˜çš„æ‰€æœ‰æ–¹é¢
- 0.7-0.9: å›ç­”äº†ä¸»è¦é—®é¢˜ï¼Œå¯èƒ½é—æ¼ç»†èŠ‚
- 0.4-0.6: éƒ¨åˆ†å›ç­”äº†é—®é¢˜ï¼Œä½†æœ‰åç¦»
- 0.0-0.3: åŸºæœ¬æ²¡æœ‰å›ç­”é—®é¢˜æˆ–å®Œå…¨è·‘é¢˜

è¯·åªè¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•°å­—ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰ï¼š"""

        response = self.llm.invoke([HumanMessage(content=prompt)])
        try:
            score = float(response.content.strip())
            return max(0.0, min(1.0, score))
        except:
            return 0.5

    def evaluate_context_precision(self, question: str, contexts: List[str]) -> float:
        """
        è¯„ä¼°æ£€ç´¢ç²¾ç¡®åº¦

        æ£€æŸ¥æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦éƒ½ä¸é—®é¢˜ç›¸å…³
        """
        if not contexts:
            return 0.0

        context_text = "\n\n---\n\n".join([f"[{i+1}] {c}" for i, c in enumerate(contexts[:5])])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ£€ç´¢ç»“æœçš„"ç²¾ç¡®åº¦"ï¼ˆContext Precisionï¼‰ã€‚

## ç”¨æˆ·é—®é¢˜
{question}

## æ£€ç´¢åˆ°çš„å†…å®¹
{context_text}

## è¯„ä¼°æ ‡å‡†
ç²¾ç¡®åº¦è¡¡é‡æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦éƒ½ä¸é—®é¢˜ç›¸å…³ã€‚
- 1.0: æ‰€æœ‰æ£€ç´¢å†…å®¹éƒ½é«˜åº¦ç›¸å…³
- 0.7-0.9: å¤§éƒ¨åˆ†å†…å®¹ç›¸å…³ï¼Œå°‘é‡ä¸å¤ªç›¸å…³
- 0.4-0.6: éƒ¨åˆ†å†…å®¹ç›¸å…³ï¼Œéƒ¨åˆ†ä¸ç›¸å…³
- 0.0-0.3: å¤§éƒ¨åˆ†å†…å®¹ä¸é—®é¢˜æ— å…³

è¯·åªè¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•°å­—ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰ï¼š"""

        response = self.llm.invoke([HumanMessage(content=prompt)])
        try:
            score = float(response.content.strip())
            return max(0.0, min(1.0, score))
        except:
            return 0.5

    def evaluate_context_recall(
        self,
        question: str,
        contexts: List[str],
        expected_answer: Optional[str] = None
    ) -> float:
        """
        è¯„ä¼°æ£€ç´¢å¬å›ç‡

        æ£€æŸ¥æ˜¯å¦æ£€ç´¢åˆ°äº†å›ç­”é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯
        éœ€è¦ ground truth answer æ¥åˆ¤æ–­
        """
        if not expected_answer:
            return 0.5  # æ— æ³•è¯„ä¼°ï¼Œè¿”å›ä¸­æ€§åˆ†æ•°

        context_text = "\n\n".join(contexts[:5])

        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ£€ç´¢è´¨é‡è¯„ä¼°ä¸“å®¶ã€‚è¯·è¯„ä¼°ä»¥ä¸‹æ£€ç´¢ç»“æœçš„"å¬å›ç‡"ï¼ˆContext Recallï¼‰ã€‚

## ç”¨æˆ·é—®é¢˜
{question}

## æ ‡å‡†ç­”æ¡ˆ
{expected_answer}

## æ£€ç´¢åˆ°çš„å†…å®¹
{context_text}

## è¯„ä¼°æ ‡å‡†
å¬å›ç‡è¡¡é‡æ£€ç´¢åˆ°çš„å†…å®¹æ˜¯å¦åŒ…å«äº†å›ç­”é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰ä¿¡æ¯ã€‚
å¯¹æ¯”æ ‡å‡†ç­”æ¡ˆï¼Œçœ‹æ£€ç´¢å†…å®¹æ˜¯å¦è¦†ç›–äº†å›ç­”æ‰€éœ€çš„å…³é”®ä¿¡æ¯ã€‚
- 1.0: æ£€ç´¢å†…å®¹å®Œå…¨è¦†ç›–äº†æ ‡å‡†ç­”æ¡ˆæ‰€éœ€çš„ä¿¡æ¯
- 0.7-0.9: è¦†ç›–äº†å¤§éƒ¨åˆ†å…³é”®ä¿¡æ¯
- 0.4-0.6: è¦†ç›–äº†éƒ¨åˆ†å…³é”®ä¿¡æ¯
- 0.0-0.3: å‡ ä¹æ²¡æœ‰è¦†ç›–å…³é”®ä¿¡æ¯

è¯·åªè¾“å‡ºä¸€ä¸ª 0 åˆ° 1 ä¹‹é—´çš„æ•°å­—ï¼ˆä¿ç•™ä¸¤ä½å°æ•°ï¼‰ï¼š"""

        response = self.llm.invoke([HumanMessage(content=prompt)])
        try:
            score = float(response.content.strip())
            return max(0.0, min(1.0, score))
        except:
            return 0.5

    def evaluate_single(
        self,
        question: str,
        answer: str,
        contexts: List[str],
        expected_answer: Optional[str] = None
    ) -> EvaluationResult:
        """è¯„ä¼°å•ä¸ªæ ·æœ¬"""
        return EvaluationResult(
            question=question,
            expected_answer=expected_answer,
            generated_answer=answer,
            contexts=contexts,
            faithfulness=self.evaluate_faithfulness(answer, contexts),
            answer_relevancy=self.evaluate_answer_relevancy(question, answer),
            context_precision=self.evaluate_context_precision(question, contexts),
            context_recall=self.evaluate_context_recall(question, contexts, expected_answer)
        )

    def evaluate_batch(
        self,
        samples: List[Dict]
    ) -> EvaluationReport:
        """
        æ‰¹é‡è¯„ä¼°

        Args:
            samples: æ ·æœ¬åˆ—è¡¨ï¼Œæ¯ä¸ªæ ·æœ¬åŒ…å«:
                - question: é—®é¢˜
                - answer: ç”Ÿæˆçš„ç­”æ¡ˆ
                - contexts: æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡åˆ—è¡¨
                - expected_answer: (å¯é€‰) æ ‡å‡†ç­”æ¡ˆ
        """
        results = []
        for sample in samples:
            result = self.evaluate_single(
                question=sample["question"],
                answer=sample["answer"],
                contexts=sample.get("contexts", []),
                expected_answer=sample.get("expected_answer")
            )
            results.append(result)
            print(f"  âœ“ è¯„ä¼°å®Œæˆ: {sample['question'][:30]}...")

        # è®¡ç®—å¹³å‡åˆ†
        n = len(results)
        return EvaluationReport(
            total_samples=n,
            avg_faithfulness=sum(r.faithfulness for r in results) / n if n else 0,
            avg_answer_relevancy=sum(r.answer_relevancy for r in results) / n if n else 0,
            avg_context_precision=sum(r.context_precision for r in results) / n if n else 0,
            avg_context_recall=sum(r.context_recall for r in results) / n if n else 0,
            results=results
        )


def print_report(report: EvaluationReport):
    """æ‰“å°è¯„ä¼°æŠ¥å‘Š"""
    print("\n" + "=" * 60)
    print("ğŸ“Š RAG è¯„ä¼°æŠ¥å‘Š")
    print("=" * 60)

    print(f"\nğŸ“ˆ æ€»ä½“æŒ‡æ ‡ ({report.total_samples} ä¸ªæ ·æœ¬)")
    print("-" * 40)
    print(f"  å¿ å®åº¦ (Faithfulness):     {report.avg_faithfulness:.2%}")
    print(f"  ç­”æ¡ˆç›¸å…³æ€§ (Relevancy):    {report.avg_answer_relevancy:.2%}")
    print(f"  æ£€ç´¢ç²¾ç¡®åº¦ (Precision):    {report.avg_context_precision:.2%}")
    print(f"  æ£€ç´¢å¬å›ç‡ (Recall):       {report.avg_context_recall:.2%}")

    print(f"\nğŸ“ è¯¦ç»†ç»“æœ")
    print("-" * 40)
    for i, r in enumerate(report.results, 1):
        print(f"\n[{i}] {r.question[:50]}...")
        print(f"    å¿ å®åº¦: {r.faithfulness:.2f} | ç›¸å…³æ€§: {r.answer_relevancy:.2f}")
        print(f"    ç²¾ç¡®åº¦: {r.context_precision:.2f} | å¬å›ç‡: {r.context_recall:.2f}")


# ============ æµ‹è¯•ç”¨ä¾‹ ============
if __name__ == "__main__":
    from src.graph_advanced import ask

    print("ğŸ§ª RAG è¯„ä¼°æ¼”ç¤º")
    print("=" * 60)

    # æµ‹è¯•é—®é¢˜
    test_questions = [
        {
            "question": "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ",
            "expected_answer": "LangGraph æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæœ‰çŠ¶æ€ã€å¤šè§’è‰²çš„ LLM åº”ç”¨çš„æ¡†æ¶ã€‚"
        },
        {
            "question": "RAG çš„æ ¸å¿ƒæ­¥éª¤æ˜¯ä»€ä¹ˆï¼Ÿ",
            "expected_answer": "RAG çš„æ ¸å¿ƒæ­¥éª¤åŒ…æ‹¬ï¼šæ£€ç´¢ï¼ˆRetrievalï¼‰ã€å¢å¼ºï¼ˆAugmentationï¼‰ã€ç”Ÿæˆï¼ˆGenerationï¼‰ã€‚"
        }
    ]

    # æ‰§è¡Œé—®ç­”å¹¶æ”¶é›†ç»“æœ
    samples = []
    for q in test_questions:
        print(f"\nâ“ æµ‹è¯•é—®é¢˜: {q['question']}")
        result = ask(q['question'], thread_id=f"eval-{hash(q['question'])}")

        contexts = []
        if result.get('local_contexts'):
            contexts.append(result['local_contexts'])
        if result.get('search_results'):
            contexts.append(result['search_results'])

        samples.append({
            "question": q['question'],
            "answer": result['answer'],
            "contexts": contexts, #æ£€ç´¢å†…å®¹
            "expected_answer": q.get('expected_answer')
        })
        print(f"   âœ“ è·å¾—ç­”æ¡ˆ: {result['answer'][:100]}...")

    # è¯„ä¼°
    print("\n" + "=" * 60)
    print("ğŸ” å¼€å§‹è¯„ä¼°...")
    evaluator = RAGEvaluator()
    report = evaluator.evaluate_batch(samples)

    # æ‰“å°æŠ¥å‘Š
    print_report(report)

    # ä¿å­˜æŠ¥å‘Š
    report_dict = {
        "total_samples": report.total_samples,
        "avg_faithfulness": report.avg_faithfulness,
        "avg_answer_relevancy": report.avg_answer_relevancy,
        "avg_context_precision": report.avg_context_precision,
        "avg_context_recall": report.avg_context_recall,
        "results": [asdict(r) for r in report.results]
    }

    with open("evaluation_report.json", "w", encoding="utf-8") as f:
        json.dump(report_dict, f, ensure_ascii=False, indent=2)

    print(f"\nğŸ’¾ æŠ¥å‘Šå·²ä¿å­˜åˆ° evaluation_report.json")
