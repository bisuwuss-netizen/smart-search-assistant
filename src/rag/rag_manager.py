"""RAG ç®¡ç†å™¨ - ç»Ÿä¸€å¯¹å¤–æ¥å£"""
import os
import hashlib
from typing import List, Dict, Set
from src.rag.document_loader import DocumentLoader
from src.rag.vector_store import VectorStore
from src.rag.retriever import HybridRetriever
from src.rag.config import RAGConfig


class RAGManager:
    _instance = None  # å•ä¾‹æ¨¡å¼

    def __init__(self):
        self.loader = DocumentLoader()
        # ä½¿ç”¨æŒä¹…åŒ–æ¨¡å¼ï¼Œæ–‡æ¡£å¯¼å…¥åé‡å¯ä¸ä¼šä¸¢å¤±
        self.vector_store = VectorStore(
            embedding_model=RAGConfig.EMBEDDING_MODEL,
            persist_dir=RAGConfig.VECTOR_DB_DIR  # å¯ç”¨æŒä¹…åŒ–
        )
        self.retriever = HybridRetriever(
            vector_store=self.vector_store,
            rerank_model=RAGConfig.RERANK_MODEL
        )

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def _compute_file_hash(self, file_path: str) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼

        ç”¨äºåˆ¤æ–­æ–‡ä»¶å†…å®¹æ˜¯å¦å·²ç»å¯¼å…¥è¿‡ï¼ˆå³ä½¿æ–‡ä»¶åç›¸åŒï¼Œå†…å®¹å˜äº†ä¹Ÿä¼šé‡æ–°å¯¼å…¥ï¼‰
        """
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            # åˆ†å—è¯»å–ï¼Œé¿å…å¤§æ–‡ä»¶å†…å­˜æº¢å‡º
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()

    def _get_indexed_sources(self) -> Set[str]:
        """
        è·å–å·²ç»ç´¢å¼•çš„æ–‡æ¡£æ¥æºé›†åˆ

        ä»å‘é‡åº“çš„ metadata ä¸­æå–æ‰€æœ‰ source å­—æ®µ
        è¿”å›æ ¼å¼: {"filename1.md:hash1", "filename2.pdf:hash2", ...}
        """
        indexed = set()
        all_docs = self.vector_store.get_all_documents()
        for doc in all_docs:
            source = doc.get("metadata", {}).get("source", "")
            file_hash = doc.get("metadata", {}).get("file_hash", "")
            if source:
                # ç”¨ "æ–‡ä»¶å:å“ˆå¸Œ" ä½œä¸ºå”¯ä¸€æ ‡è¯†
                filename = os.path.basename(source)
                indexed.add(f"{filename}:{file_hash}")
        return indexed

    def is_document_indexed(self, file_path: str) -> bool:
        """
        æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ç»è¢«ç´¢å¼•

        åˆ¤æ–­é€»è¾‘ï¼šæ–‡ä»¶å + æ–‡ä»¶å†…å®¹å“ˆå¸Œ éƒ½åŒ¹é…æ‰ç®—å·²ç´¢å¼•
        è¿™æ ·å³ä½¿æ–‡ä»¶åç›¸åŒä½†å†…å®¹å˜äº†ï¼Œä¹Ÿä¼šé‡æ–°å¯¼å…¥
        """
        filename = os.path.basename(file_path)
        file_hash = self._compute_file_hash(file_path)
        identifier = f"{filename}:{file_hash}"

        indexed_sources = self._get_indexed_sources()
        return identifier in indexed_sources

    def add_document(self, file_path: str, force: bool = False) -> int:
        """
        æ·»åŠ å•ä¸ªæ–‡æ¡£åˆ°çŸ¥è¯†åº“

        Args:
            file_path: æ–‡æ¡£è·¯å¾„
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥ï¼ˆå³ä½¿å·²å­˜åœ¨ï¼‰

        Returns:
            å¯¼å…¥çš„ chunk æ•°é‡ï¼Œå¦‚æœå·²å­˜åœ¨åˆ™è¿”å› 0
        """
        filename = os.path.basename(file_path)

        # å»é‡æ£€æŸ¥
        if not force and self.is_document_indexed(file_path):
            print(f"â­ï¸  è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {filename}")
            return 0

        # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼Œç”¨äºåç»­å»é‡åˆ¤æ–­
        file_hash = self._compute_file_hash(file_path)

        # åŠ è½½å¹¶åˆ‡åˆ†æ–‡æ¡£
        chunks = self.loader.load_and_split(
            file_path,
            chunk_size=RAGConfig.CHUNK_SIZE,
            overlap=RAGConfig.CHUNK_OVERLAP
        )

        documents = [chunk["content"] for chunk in chunks]
        # åœ¨ metadata ä¸­æ·»åŠ  file_hashï¼Œç”¨äºå»é‡
        metadatas = []
        for chunk in chunks:
            meta = chunk["metadata"].copy()
            meta["file_hash"] = file_hash  # æ·»åŠ å“ˆå¸Œå€¼
            metadatas.append(meta)

        return self.vector_store.add_documents(documents, metadatas)

    def add_documents_from_dir(self, dir_path: str, force: bool = False) -> int:
        """
        æ‰¹é‡æ·»åŠ ç›®å½•ä¸‹çš„æ–‡æ¡£

        Args:
            dir_path: æ–‡æ¡£ç›®å½•è·¯å¾„
            force: æ˜¯å¦å¼ºåˆ¶é‡æ–°å¯¼å…¥æ‰€æœ‰æ–‡æ¡£

        Returns:
            æ–°å¯¼å…¥çš„ chunk æ€»æ•°
        """
        total_added = 0
        skipped = 0
        supported_extensions = ('.pdf', '.txt', '.md')

        files = [f for f in os.listdir(dir_path) if f.endswith(supported_extensions)]

        if not files:
            print(f"âš ï¸  ç›®å½•ä¸ºç©ºæˆ–æ— æ”¯æŒçš„æ–‡ä»¶: {dir_path}")
            return 0

        print(f"ğŸ“‚ æ‰«æåˆ° {len(files)} ä¸ªæ–‡æ¡£")

        for filename in files:
            file_path = os.path.join(dir_path, filename)
            try:
                count = self.add_document(file_path, force=force)
                if count > 0:
                    total_added += count
                    print(f"âœ… å·²æ·»åŠ : {filename} ({count} chunks)")
                else:
                    skipped += 1
            except Exception as e:
                print(f"âŒ æ·»åŠ å¤±è´¥: {filename} - {e}")

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š å¯¼å…¥ç»Ÿè®¡: æ–°å¢ {total_added} chunks, è·³è¿‡ {skipped} ä¸ªå·²å­˜åœ¨æ–‡æ¡£")
        return total_added

    def query(self, question: str, top_n: int = 5) -> Dict:
        """æ£€ç´¢å¹¶è¿”å›ç›¸å…³æ–‡æ¡£"""
        # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦ä¸ºç©º
        if self.vector_store.count() == 0:
            return {
                "contexts": [],
                "formatted": "## æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç»“æœ\n\næš‚æ— æ–‡æ¡£ï¼Œè¯·å…ˆæ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ã€‚"
            }

        contexts = self.retriever.retrieve(
            question,
            top_k=RAGConfig.VECTOR_SEARCH_TOP_K,
            top_n=top_n,
            vector_weight=RAGConfig.VECTOR_WEIGHT
        )
        return {
            "contexts": contexts,
            "formatted": self._format_contexts(contexts)
        }

    def _format_contexts(self, contexts: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ£€ç´¢ç»“æœï¼Œç”¨äº Prompt"""
        if not contexts:
            return "## æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç»“æœ\n\næœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"

        result = "## æœ¬åœ°çŸ¥è¯†åº“æ£€ç´¢ç»“æœ\n\n"
        for i, ctx in enumerate(contexts, 1):
            source = ctx.get('metadata', {}).get('source', 'æœªçŸ¥æ¥æº')
            # åªæ˜¾ç¤ºæ–‡ä»¶åï¼Œä¸æ˜¾ç¤ºå®Œæ•´è·¯å¾„
            if source and source != 'æœªçŸ¥æ¥æº':
                source = os.path.basename(source)
            score = ctx.get('score', 0)
            result += f"[{i}] æ¥æº: {source} (ç›¸å…³åº¦: {score:.2f})\n"
            result += f"å†…å®¹: {ctx['content']}\n\n"
        return result

    def clear(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        self.vector_store.clear()

    def count(self) -> int:
        """è·å–çŸ¥è¯†åº“æ–‡æ¡£æ•°é‡"""
        return self.vector_store.count()

    def list_documents(self) -> List[str]:
        """
        åˆ—å‡ºå·²ç´¢å¼•çš„æ–‡æ¡£

        Returns:
            æ–‡æ¡£ååˆ—è¡¨ï¼ˆå»é‡åï¼‰
        """
        all_docs = self.vector_store.get_all_documents()
        sources = set()
        for doc in all_docs:
            source = doc.get("metadata", {}).get("source", "")
            if source:
                sources.add(os.path.basename(source))
        return sorted(list(sources))
